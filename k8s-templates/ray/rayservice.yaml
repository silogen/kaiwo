apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm-distributed-inference-test
  namespace: av-test
spec:
  serveConfigV2: |
    applications:
    - name: llm
      route_prefix: /
      import_path: workload:deployment
      deployments:
      - name: VLLMDeployment
        autoscaling_config:
          metrics_interval_s: 0.2
          min_replicas: 1
          max_replicas: 4
          look_back_period_s: 2
          downscale_delay_s: 600
          upscale_delay_s: 30
          target_num_ongoing_requests_per_replica: 20
        graceful_shutdown_timeout_s: 5
        max_concurrent_queries: 100
      runtime_env:
        env_vars:
          MODEL_ID: "meta-llama/Llama-3.1-8B-Instruct"
          TENSOR_PARALLELISM: "1"
          PIPELINE_PARALLELISM: "1"
          GPU_MEMORY_UTILIZATION: "0.9"
          PLACEMENT_STRATEGY: "PACK"
          MAX_MODEL_LEN: "8192"
          MAX_NUM_SEQ: "4"
          MAX_NUM_BATCHED_TOKENS: "32768"
  rayClusterConfig:
    enableInTreeAutoscaling: true
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        spec:
          containers:
            - name: ray-head
              image: ghcr.io/silogen/rocm-ray:v0.3
              env:
                - name: HUGGING_FACE_HUB_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: hf-token
                      key: hf-token
              resources:
                limits:
                  cpu: "10"
                  memory: "300Gi"
                  # amd.com/gpu: "8"
                requests:
                  cpu: "10"
                  memory: "300Gi"
                  # amd.com/gpu: "8"
              volumeMounts:
                - mountPath: /workload
                  name: app-mount
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
          volumes:
            - name: app-mount
              configMap:
                name: vllm-serve-example
    workerGroupSpecs:
      - replicas: 1
        minReplicas: 1
        maxReplicas: 4
        groupName: gpu-group
        rayStartParams: {}
        template:
          spec:
            containers:
              - name: llm
                image: ghcr.io/silogen/rocm-ray:v0.3
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "ray stop"]
                env:
                  - name: HUGGING_FACE_HUB_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-token
                        key: hf-token
                resources:
                  limits:
                    cpu: "10"
                    memory: "300Gi"
                    amd.com/gpu: "8"
                  requests:
                    cpu: "10"
                    memory: "300Gi"
                    amd.com/gpu: "8"
                volumeMounts:
                  - mountPath: /workload
                    name: app-mount
            volumes:
              - name: app-mount
                configMap:
                  name: vllm-serve-example
