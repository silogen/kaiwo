APIVersion: v1
Data:
  entrypoint: |-
    |
      python main.py
      --model-name=meta-llama/Llama-3.1-8B-Instruct
      --ds-config=./zero_3_offload_optim_param.json
      --bucket=silogen-dev-ray
      --num-epochs=2
      --lora
      --num-devices=16
      --batch-size-per-device=32
      --eval-batch-size-per-device=32
      --ctx-len=1024
  lora-llama.json: "|\n  {   \n      \"r\": 8,\n      \"lora_alpha\": 16,\n      \"lora_dropout\":
    0.05,\n      \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",
    \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n      \"task_type\":
    \"CAUSAL_LM\",\n      \"modules_to_save\": [],\n      \"bias\": \"none\",\n      \"fan_in_fan_out\":
    false,\n      \"init_lora_weights\": true\n  }"
  main.py: "|\n  # This code includes portions from distributed-workloads, licensed
    under the Apache License 2.0.\n  # See https://github.com/opendatahub-io/distributed-workloads.git
    for details.\n  \n  # Copyright 2025 Advanced Micro Devices, Inc. All rights reserved.\n
    \ # Licensed under the Apache License, Version 2.0 (the \"License\");\n  # you
    may not use this file except in compliance with the License.\n  # You may obtain
    a copy of the License at\n  \n  # http://www.apache.org/licenses/LICENSE-2.0\n
    \ \n  # Unless required by applicable law or agreed to in writing, software\n
    \ # distributed under the License is distributed on an \"AS IS\" BASIS,\n  # WITHOUT
    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the
    License for the specific language governing permissions and\n  # limitations under
    the License.\n  \n  import argparse\n  from filelock import FileLock\n  import
    functools\n  import json\n  import math\n  import os\n  from pathlib import Path\n
    \ import tempfile\n  import time\n  import tree\n  from typing import Tuple\n
    \ import urllib\n  from urllib.parse import urljoin\n  \n  import deepspeed \n
    \ \n  from accelerate import Accelerator, DeepSpeedPlugin\n  from accelerate.utils
    import DummyOptim, DummyScheduler, set_seed\n  import torch\n  import torch.nn
    as nn\n  import tqdm\n  from datasets import load_dataset\n  from transformers
    import (\n      AutoModelForCausalLM,\n      AutoTokenizer,\n      get_linear_schedule_with_warmup,\n
    \ )\n  \n  from pyarrow.fs import S3FileSystem, AwsDefaultS3RetryStrategy, GcsFileSystem\n
    \ \n  from peft import LoraConfig, get_peft_model\n  import ray\n  from ray import
    train\n  import ray.util.scheduling_strategies\n  from ray.train.torch import
    TorchTrainer\n  from ray.train import Checkpoint\n  \n  \n  \n  urllib.parse.uses_relative.append(\"s3\")\n
    \ urllib.parse.uses_netloc.append(\"s3\")\n  \n  OPTIM_BETAS = (0.9, 0.999)\n
    \ OPTIM_EPS = 1e-8\n  NUM_WARMUP_STEPS = 10\n  OPTIM_WEIGHT_DECAY = 0.0\n  ATTENTION_LAYER_NAME
    = \"self_attn\"\n  \n  def load_gsm8k_dataset():\n      \"\"\"Loads the GSM8K
    dataset.\"\"\"\n      cache_dir = \"../../datasets\"\n      if not os.path.exists(cache_dir):\n
    \         cache_dir = \"\"\n      return load_dataset(\"gsm8k\", \"main\", cache_dir=cache_dir)\n
    \ \n  \n  def process_gsm8k_qa_tokens_template(dataset):\n      \"\"\"Processes
    GSM8K dataset with QA tokens template.\"\"\"\n      config = {\n          \"chat_template\":
    \"{{ messages }}\",\n          \"special_tokens\": [\"<START_Q>\", \"<END_Q>\",
    \"<START_A>\", \"<END_A>\"],\n      }\n  \n      processed_data = {}\n      for
    split, ds in dataset.items():\n          processed_data[split] = [\n              {\n
    \                 \"messages\": (\n                      f\"<START_Q>{item['question']}<END_Q>\"\n
    \                     f\"<START_A>{item['answer']}<END_A>\"\n                  )\n
    \             }\n              for item in ds\n          ]\n  \n      return config,
    processed_data\n  \n  \n  def process_gsm8k_qa_no_tokens_template(dataset):\n
    \     \"\"\"Processes GSM8K dataset with QA no tokens template.\"\"\"\n      config
    = {\n          \"chat_template\": (\n              \"{% for message in messages
    %}\"\n              \"{% if message['role'] == 'system' %}\"\n              \"{{
    message['content'] }}\"\n              \"{% elif message['role'] == 'user' %}\"\n
    \             \"{{ '\\n\\nQuestion: ' + message['content'] +  eos_token }}\"\n
    \             \"{% elif message['role'] == 'assistant' %}\"\n              \"{{
    '\\n\\nAnswer: '  + message['content'] +  eos_token  }}\"\n              \"{%
    endif %}\"\n              \"{% endfor %}\"\n              \"{% if add_generation_prompt
    %}\"\n              \"{{ '\\n\\nAnswer: ' }}\"\n              \"{% endif %}\"\n
    \         )\n      }\n  \n      processed_data = {}\n      for split, ds in dataset.items():\n
    \         processed_data[split] = [\n              {\n                  \"messages\":
    [\n                      {\"role\": \"user\", \"content\": item[\"question\"]},\n
    \                     {\"role\": \"assistant\", \"content\": item[\"answer\"]},\n
    \                 ]\n              }\n              for item in ds\n          ]\n
    \ \n      return config, processed_data\n  \n  \n  def process_gsm8k_hf_chat_template(dataset):\n
    \     \"\"\"Processes GSM8K dataset for Hugging Face chat template.\"\"\"\n      processed_data
    = {}\n      for split, ds in dataset.items():\n          processed_data[split]
    = [\n              {\n                  \"messages\": [\n                      {\"role\":
    \"user\", \"content\": item[\"question\"]},\n                      {\"role\":
    \"assistant\", \"content\": item[\"answer\"]},\n                  ]\n              }\n
    \             for item in ds\n          ]\n  \n      return processed_data\n  \n
    \ def get_expected_lora_num_parameters(\n      model, lora_config: LoraConfig,
    attn_layer_name: str = ATTENTION_LAYER_NAME\n  ):\n      \"\"\"Calculate the expected
    number of parameters for lora fine-tuning.\"\"\"\n      sum_params = 0\n      num_attention_layers
    = 0\n      modules = model.named_modules()\n      loraified_modules = 0\n      #
    We calculate the number of parameters we need for lora fine-tuning by calculating\n
    \     # the sizes of the decomposed weight matrices according to the paper.\n
    \     for full_name, target in modules:\n          layer_name = full_name.split(\".\")[-1]\n
    \ \n          if layer_name == attn_layer_name:\n              # Detected another
    attention layer (for example, llama 2 70b should have 80\n              # of these)\n
    \             num_attention_layers += 1\n          elif layer_name in lora_config.modules_to_save:\n
    \             # Detect another non-lora module to save, which will also contribute
    to the\n              # number of check-pointed parameters. This will result in
    one set of\n              # trainable parameters \"<layer>.original_module.weight\"
    and another one with\n              # \"<layer>.modules_to_save.default.weight\"\n
    \             # Therefore, each layer contributes 2 x the number of actual elements
    in\n              # that layer.\n              sum_params += 2 * target.weight.numel()\n
    \             print(\n                  \"Found non-lora-layer to checkpoint:
    \",\n                  layer_name,\n                  \" with num params \",\n
    \                 target.weight.numel(),\n              )\n          else:\n              for
    module_name in lora_config.target_modules:\n                  if layer_name ==
    module_name:\n                      loraified_modules += 1\n                      if
    isinstance(target, nn.Linear):\n                          # Target is attention
    weight\n                          sum_params += (\n                              target.in_features
    + target.out_features\n                          ) * lora_config.r\n                      elif
    isinstance(target, nn.Embedding):\n                          # Target is linear
    weight\n                          sum_params += (\n                              target.embedding_dim
    + target.num_embeddings\n                          ) * lora_config.r\n  \n      print(\n
    \         f\"Detected {num_attention_layers} attention layers, containing\"\n
    \         f\" {loraified_modules} modules to modify according to LoRA's `target_modules`.\"\n
    \         f\" This should yield {sum_params} trainable parameters.\"\n      )\n
    \ \n      return sum_params\n  \n  \n  def get_number_of_params(model: nn.Module):\n
    \     sum = 0\n      for name, param in model.named_parameters():\n          if
    param.requires_grad:\n              sum += param.numel()\n      return sum\n  \n
    \ \n  def collate_fn(batch, tokenizer, block_size, device):\n      out_batch =
    tokenizer(\n          list(map(lambda m: tokenizer.apply_chat_template(m,\n                                                          tokenize=False,\n
    \                                                         add_generation_prompt=False,\n
    \                                                         add_special_tokens=False),\n
    \                 batch[\"messages\"])),\n          padding=\"max_length\",\n
    \         max_length=block_size,\n          truncation=True,\n          return_tensors=\"pt\",\n
    \     )\n      out_batch[\"labels\"] = out_batch[\"input_ids\"].clone()\n  \n
    \     out_batch = tree.map_structure(lambda x: x.to(device), out_batch)\n  \n
    \     return out_batch\n  \n  \n  \n  \n  def get_tokenizer(model_name, **kwargs):\n
    \     # Context for legacy=True: https://github.com/huggingface/transformers/issues/25176\n
    \     tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=True)\n      tokenizer.pad_token
    = tokenizer.eos_token\n      if kwargs.get(\"special_tokens\", None):\n          tokenizer.add_tokens(kwargs.get(\"special_tokens\"),
    special_tokens=True)\n      if kwargs.get(\"chat_template\", None):\n          tokenizer.chat_template
    = kwargs.get(\"chat_template\")\n  \n      return tokenizer\n  \n  \n  def evaluate(\n
    \     *, model, eval_ds, accelerator, bsize, ds_kwargs, as_test: bool = False\n
    \ ) -> Tuple[float, float]:\n      model.eval()\n      losses = []\n  \n      eval_dataloader
    = eval_ds.iter_torch_batches(batch_size=bsize, **ds_kwargs)\n      eval_ds_len
    = len(list(eval_ds.iter_batches(batch_size=1)))\n      for step, batch in tqdm.tqdm(\n
    \         enumerate(eval_dataloader), total=eval_ds_len // (bsize + 1)\n      ):\n
    \         with torch.no_grad():\n              outputs = model(**batch)\n  \n
    \         loss = outputs.loss\n          # The tensors are gathered by concatenating
    them on the first dimension, so we\n          # add a new dimension to the scalar
    loss to get a tensor of shape (K,) for K\n          # workers.\n          losses.append(accelerator.gather(loss[None]))\n
    \ \n          if as_test:\n              break\n  \n      # We stack losses so
    that we have a tensor of shape (T, K) where T is the number of\n      # steps
    and K is the number of workers.\n      losses = torch.stack(losses)\n      try:\n
    \         eval_loss = torch.mean(losses).item()\n          perplexity = math.exp(eval_loss)\n
    \     except OverflowError:\n          perplexity = float(\"inf\")\n      return
    perplexity, eval_loss\n  \n  \n  def _test_tokenizer(model_name):\n      # This
    function tests that adding special tokens does not\n      # result in un-expected
    tokenization\n      # Context: https://github.com/huggingface/transformers/issues/25176\n
    \     tokenizer = get_tokenizer(model_name=model_name, special_tokens=[\"<REPR_END>\"])\n
    \     testoutput = tokenizer(\"<REPR_END>inform\")[\"input_ids\"]\n      expected
    = tokenizer(\"inform\")[\"input_ids\"]\n      assert testoutput[-1] == expected[-1],
    (\n          \"The tokenizer is not working as expected with special tokens, \"\n
    \         f\"testoutput={testoutput}, expected={expected}\"\n      )\n  \n  \n
    \ def checkpoint_model(\n      checkpoint_folder, ckpt_id, model, epoch, last_global_step,
    **kwargs\n  ):\n      \"\"\"Utility function for checkpointing model + optimizer
    dictionaries\n      The main purpose for this is to be able to resume training
    from that instant again.\n      \"\"\"\n      checkpoint_state_dict = {\n          \"epoch\":
    epoch,\n          \"last_global_step\": last_global_step,\n      }\n      # Add
    extra kwargs too\n      checkpoint_state_dict.update(kwargs)\n  \n      # In here
    model will be a DeepspeedEngine object\n      model.save_checkpoint(checkpoint_folder,
    ckpt_id, checkpoint_state_dict)\n      status_msg = (\n          f\"checkpointing:
    checkpoint_folder={checkpoint_folder}, ckpt_id={ckpt_id}\"\n      )\n      print(status_msg)\n
    \ \n  \n  def training_function(kwargs: dict):\n      print(\"training_function
    called\")\n  \n      # Train has a bug somewhere that causes ACCELERATE_TORCH_DEVICE
    to not be set\n      # properly on multi-gpu nodes\n      cuda_visible_device
    = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n      local_rank = int(os.environ[\"LOCAL_RANK\"])\n
    \     device_id = cuda_visible_device[local_rank]\n      os.environ[\"ACCELERATE_TORCH_DEVICE\"]
    = f\"cuda:{device_id}\"\n  \n      config = kwargs[\"config\"]\n      args = argparse.Namespace(**kwargs[\"args\"])\n
    \     chat_template = kwargs.get(\"chat_template\", [])\n      special_tokens
    = kwargs.get(\"special_tokens\", [])\n      model_id = config[\"model_name\"]\n
    \ \n  \n      # Sample hyperparameters for learning rate, batch size, seed and
    a few other HPs\n      lr = config[\"lr\"]\n      num_epochs = int(config[\"num_epochs\"])\n
    \     seed = int(config[\"seed\"])\n      batch_size = int(config[\"batch_size\"])\n
    \     gradient_accumulation_steps = int(config[\"gradient_accumulation_steps\"])\n
    \ \n      # Get deepspeed config to set up the batch size per device\n      ds_plugin
    = config[\"ds_plugin\"]\n      ds_plugin.hf_ds_config.config[\"train_micro_batch_size_per_gpu\"]
    = batch_size\n  \n      # Initialize accelerator\n      accelerator = Accelerator(\n
    \         deepspeed_plugin=ds_plugin,\n          gradient_accumulation_steps=gradient_accumulation_steps,\n
    \         mixed_precision=args.mx,\n      )\n  \n      set_seed(seed)\n  \n      #
    train_ds is the local shard for this model\n      train_ds = train.get_dataset_shard(\"train\")\n
    \     valid_ds = train.get_dataset_shard(\"valid\")\n  \n      train_ds_len =
    len(list(train_ds.iter_batches(batch_size=1)))\n  \n      _test_tokenizer(args.model_name)\n
    \     tokenizer = get_tokenizer(model_name=args.model_name,\n                                  chat_template=chat_template,\n
    \                                 special_tokens=special_tokens)\n      collate_partial
    = functools.partial(\n          collate_fn,\n          tokenizer=tokenizer,\n
    \         block_size=config[\"block_size\"],\n          device=accelerator.device,\n
    \     )\n  \n      s = time.time()\n      model = AutoModelForCausalLM.from_pretrained(\n
    \         args.model_name,\n          trust_remote_code=True,\n          torch_dtype=torch.bfloat16,\n
    \         # `use_cache=True` is incompatible with gradient checkpointing.\n          use_cache=False,\n
    \         attn_implementation=\"flash_attention_2\",\n      )\n      print(f\"Done
    loading model in {time.time() - s} seconds.\")\n  \n      model.resize_token_embeddings(len(tokenizer))\n
    \ \n      if config[\"lora\"]:\n          # Apply LoRA\n          s = time.time()\n
    \         lora_config = LoraConfig(**config[\"lora_config\"])\n  \n          expected_num_parameters
    = get_expected_lora_num_parameters(\n              lora_config=lora_config, model=model\n
    \         )\n  \n          print(f\"Attempting to apply LoRA config: {lora_config}\")\n
    \ \n          model.enable_input_require_grads()\n          model = get_peft_model(model,
    lora_config)\n  \n          num_parameters = get_number_of_params(model)\n  \n
    \         if num_parameters != expected_num_parameters:\n              raise ValueError(\n
    \                 f\"Expected {expected_num_parameters} parameters, got {num_parameters}
    \"\n                  f\"parameters. LoRA-ification failed.\"\n              )\n
    \ \n          print(\n              f\"LoRA-ification done in {time.time() - s}
    seconds. Estimated checkpoint \"\n              f\"size (fp16): {num_parameters
    * 2 / 1e6} MB\"\n          )\n  \n      print(f\"Number of check-pointed parameters:
    {get_number_of_params(model)}\")\n  \n      print(\"Model initialized with pretrained
    weights. Training starting...\")\n      if not args.no_grad_ckpt:\n          model.gradient_checkpointing_enable()\n
    \ \n      optimizer_cls = (\n          torch.optim.AdamW\n          if accelerator.state.deepspeed_plugin
    is None\n          or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n
    \         else DummyOptim\n      )\n  \n      optimizer = optimizer_cls(\n          model.parameters(),\n
    \         lr=lr,\n          betas=OPTIM_BETAS,\n          weight_decay=OPTIM_WEIGHT_DECAY,\n
    \         eps=OPTIM_EPS,\n      )\n  \n      # Instantiate scheduler\n      #
    Creates Dummy Scheduler if `scheduler` was specified in the config file or\n      #
    else, creates `args.lr_scheduler_type` Scheduler\n      # get train and valid
    dataset lengths\n  \n      num_steps_per_epoch = math.ceil(train_ds_len / args.batch_size_per_device)\n
    \     total_training_steps = (\n          num_steps_per_epoch * num_epochs //
    gradient_accumulation_steps\n      )\n  \n      if (\n          accelerator.state.deepspeed_plugin
    is None\n          or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n
    \     ):\n          lr_scheduler = get_linear_schedule_with_warmup(\n              optimizer=optimizer,\n
    \             num_warmup_steps=NUM_WARMUP_STEPS * args.num_devices,\n              num_training_steps=total_training_steps
    * args.num_devices,\n          )\n      else:\n          lr_scheduler = DummyScheduler(\n
    \             optimizer,\n              warmup_num_steps=NUM_WARMUP_STEPS * args.num_devices,\n
    \             total_num_steps=total_training_steps * args.num_devices,\n          )\n
    \ \n      # Prepare everything\n      # There is no specific order to remember,
    we just need to unpack the objects in the\n      # same order we gave them to
    the prepare method.\n      s = time.time()\n      model, optimizer, lr_scheduler
    = accelerator.prepare(model, optimizer, lr_scheduler)\n      print(f\"Prepare
    done in {time.time() - s} seconds.\")\n  \n      # Now we train the model\n      if
    accelerator.is_main_process:\n          print(\"Starting training ...\")\n          print(\"Number
    of batches on main process\", train_ds_len // batch_size)\n  \n      for epoch
    in range(num_epochs):\n          fwd_time_sum, bwd_time_sum, optim_step_time_sum
    = 0, 0, 0\n          s_epoch = time.time()\n          model.train()\n          loss_sum
    = torch.tensor(0.0).to(accelerator.device)\n  \n          train_dataloader = train_ds.iter_torch_batches(\n
    \             batch_size=batch_size,\n              collate_fn=collate_partial,\n
    \         )\n  \n          for step, batch in tqdm.tqdm(\n              enumerate(train_dataloader),
    total=train_ds_len // batch_size + 1\n          ):\n  \n              # We could
    avoid this line since we set the accelerator with\n              # `device_placement=True`.\n
    \             with accelerator.accumulate(model):\n                  s_fwd = time.time()\n
    \                 outputs = model(**batch)\n                  loss = outputs.loss\n
    \                 loss_sum += loss.item()\n                  e_fwd = time.time()\n
    \                 fwd_time = e_fwd - s_fwd\n                  fwd_time_sum +=
    fwd_time\n                  s_bwd = time.time()\n                  accelerator.backward(loss)\n
    \                 e_bwd = time.time()\n                  bwd_time = e_bwd - s_bwd\n
    \                 bwd_time_sum += bwd_time\n  \n                  s_opt_step =
    time.time()\n                  optimizer.step()\n                  lr_scheduler.step()\n
    \                 optimizer.zero_grad()\n                  e_opt_step = time.time()\n
    \                 optim_step_time_sum += e_opt_step - s_opt_step\n  \n              if
    accelerator.is_main_process:\n                  accelerator.print(\n                      f\"[epoch
    {epoch} step {step}] \"\n                      f\"loss: {loss.item()} step-time:
    {e_opt_step - s_fwd}\"\n                  )\n  \n              aggregated_loss
    = torch.mean(accelerator.gather(loss[None])).item()\n  \n              if config[\"as_test\"]:\n
    \                 break\n  \n              # as long as this is not the last step
    report here\n              if step != (train_ds_len // batch_size - 1):\n                  train.report(\n
    \                     {\n                          \"epoch\": epoch,\n                          \"iteration\":
    step,\n                          \"train_loss_batch\": aggregated_loss,\n                          \"avg_train_loss_epoch\":
    None,\n                          \"eval_loss\": None,\n                          \"perplexity\":
    None,\n                          \"num_iterations\": step + 1,\n                          \"train_time_per_epoch\":
    None,\n                          \"eval_time_per_epoch\": None,\n                          \"fwd_time\":
    fwd_time,\n                          \"bwd_time\": bwd_time,\n                          \"avg_fwd_time_per_epoch\":
    None,\n                          \"avg_bwd_time_per_epoch\": None,\n                          \"learning_rate\":
    lr_scheduler.get_lr()[0],\n                      }\n                  )\n  \n
    \         e_epoch = time.time()\n          accelerator.print(\"Train time per
    epoch: \", e_epoch - s_epoch)\n  \n          eval_s_epoch = time.time()\n          print(\"Running
    evaluation ...\")\n          perplex, eloss = evaluate(\n              model=model,\n
    \             eval_ds=valid_ds,\n              accelerator=accelerator,\n              bsize=config[\"eval_batch_size\"],\n
    \             ds_kwargs={\"collate_fn\": collate_partial},\n              as_test=config[\"as_test\"],\n
    \         )\n          accelerator.print(\"Eval result loss\", eloss)\n          accelerator.print(\"Eval
    perplex\", perplex)\n  \n          eval_e_epoch = time.time()\n          accelerator.print(\"Eval
    time per epoch: \", eval_e_epoch - eval_s_epoch)\n          accelerator.print(\"avg
    fwd time: \", fwd_time_sum / (step + 1))\n          accelerator.print(\"avg bwd
    time: \", bwd_time_sum / (step + 1))\n          accelerator.print(\"avg opt step
    time: \", optim_step_time_sum / (step + 1))\n  \n          metrics = {\n              \"epoch\":
    epoch,\n              \"iteration\": step,\n              \"train_loss_batch\":
    aggregated_loss,\n              \"avg_train_loss_epoch\": loss_sum.item() / (step
    + 1),\n              \"eval_loss\": eloss,\n              \"perplexity\": perplex,\n
    \             \"num_iterations\": step + 1,\n              \"train_time_per_epoch\":
    e_epoch - s_epoch,\n              \"eval_time_per_epoch\": eval_e_epoch - eval_s_epoch,\n
    \             \"fwd_time\": fwd_time,\n              \"bwd_time\": bwd_time,\n
    \             \"avg_fwd_time_per_epoch\": fwd_time_sum / (step + 1),\n              \"avg_bwd_time_per_epoch\":
    bwd_time_sum / (step + 1),\n              \"learning_rate\": lr_scheduler.get_lr()[0],\n
    \         }\n  \n          with tempfile.TemporaryDirectory(dir=args.output_dir)
    as temp_checkpoint_dir:\n              accelerator.print(f\"Saving the model locally
    at {temp_checkpoint_dir}\")\n              accelerator.wait_for_everyone()\n  \n
    \             checkpoint_save_start = time.perf_counter()\n  \n              if
    accelerator.is_main_process:\n                  print(\"Saving tokenizer and config.\")\n
    \                 tokenizer.save_pretrained(temp_checkpoint_dir)\n  \n              accelerator.wait_for_everyone()\n
    \ \n              # Checkpointing strategy 1: Distributed checkpointing\n              #
    This checkpointing method makes deepspeed checkpoints on each node\n              #
    and then Ray Train will aggregate them to a central s3 bucket.\n              #
    It should be done on all processes (not just the Rank 0)\n              # aggregate_on_rank_0
    = False\n              # checkpoint_model(\n              #     checkpoint_folder=tempdir,\n
    \             #     ckpt_id=epoch,\n              #     model=model,\n              #
    \    epoch=epoch,\n              #     last_global_step=step\n              #
    )\n  \n              # Checkpointing strategy 2: Aggregate model on the rank 0
    worker then upload\n              aggregate_on_rank_0 = True\n              unwrapped_model
    = accelerator.unwrap_model(model)\n              unwrapped_model.save_pretrained(\n
    \                 temp_checkpoint_dir,\n                  is_main_process=accelerator.is_main_process,\n
    \                 save_function=accelerator.save,\n                  safe_serialization=True,\n
    \                 state_dict=accelerator.get_state_dict(model),\n              )\n
    \             accelerator.wait_for_everyone()\n              print(\"Checkpoint
    save time: \", time.perf_counter() - checkpoint_save_start)\n  \n              checkpoint_upload_start
    = time.perf_counter()\n  \n              # Create the checkpoint object to report
    to Ray Train and upload to storage.\n              # If we aggregated the model
    on rank 0, we only need to report\n              # the checkpoint from the rank
    0 worker, since all other checkpoint\n              # directories are empty (`save_pretrained`
    was a noop for other workers).\n              if aggregate_on_rank_0:\n                  checkpoint
    = (\n                      Checkpoint.from_directory(temp_checkpoint_dir)\n                      if
    accelerator.is_main_process\n                      else None\n                  )\n
    \             else:\n                  # Distributed checkpointing should upload
    shards from each worker.\n                  checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n
    \ \n              # Note: After `train.report`, in the case of remote storage,\n
    \             # the checkpoint directory will be uploaded to the remote storage.\n
    \             train.report(metrics, checkpoint=checkpoint)\n  \n              print(\n
    \                 \"Checkpoint upload time: \",\n                  time.perf_counter()
    - checkpoint_upload_start,\n              )\n              print(\n                  \"Total
    checkpointing time: \",\n                  time.perf_counter() - checkpoint_save_start,\n
    \             )\n  \n          if perplex < args.stop_perplexity:\n              print(f\"Perplexity
    reached {perplex} < {args.stop_perplexity}. Stopping.\")\n              break\n
    \ \n          if config[\"as_test\"]:\n              break\n  \n  \n  def parse_args():\n
    \     parser = argparse.ArgumentParser(description=\"LLM fine-tuning with DeepSpeed\")\n
    \ \n      parser.add_argument(\"--model-name\", type=str, default=\"meta-llama/Meta-Llama-3.1-8B\")\n
    \ \n      parser.add_argument(\"--train-path\", type=str, default=\"./data/train.jsonl\",\n
    \                         help=\"Path to training jsonl file\")\n  \n      parser.add_argument(\"--test-path\",
    type=str, default=\"./data/test.jsonl\",\n                          help=\"Path
    to testing jsonl file\")\n  \n      parser.add_argument(\"--dataset-config\",
    type=str, default=\"./data/config.json\",\n                          help=\"Path
    to the config file\")\n  \n      parser.add_argument(\"--num-devices\", \"-nd\",
    type=int, default=4,\n                          help=\"Number of devices to use.\")\n
    \ \n      parser.add_argument(\"--mx\", type=str, choices=[\"no\", \"fp16\", \"bf16\",
    \"fp8\"], default=\"bf16\",\n                          help=\"Whether to use mixed
    precision. Choose between fp16 and bf16 (bfloat16). \"\n                              \"Bf16
    requires PyTorch >= 1.10 and an Nvidia Ampere GPU.\")\n  \n      parser.add_argument(\"--ds-config\",
    type=str, default=\"./deepspeed_configs/zero_3_offload_optim_param.json\",\n                          help=\"Deepspeed
    config json to use.\")\n  \n      parser.add_argument(\"--lora\", action=\"store_true\",
    default=False,\n                          help=\"If passed, will enable parameter
    efficient fine-tuning with LoRA.\")\n  \n      parser.add_argument(\"--lora-config\",
    type=str, default=\"./lora-llama.json\",\n                          help=\"Lora
    config json to use.\")\n  \n      parser.add_argument(\"--num-epochs\", type=int,
    default=1,\n                          help=\"Number of epochs to train for.\")\n
    \ \n      parser.add_argument(\"--lr\", type=float, default=1e-4,\n                          help=\"Learning
    rate to use.\")\n  \n      parser.add_argument(\"--ctx-len\", type=int, default=512,\n
    \                         help=\"Maximum context length for the model input sequences.\")\n
    \ \n      parser.add_argument(\"--batch-size-per-device\", \"-bs\", type=int,
    default=16,\n                          help=\"Batch size to use per device.\")\n
    \ \n      parser.add_argument(\"--eval-batch-size-per-device\", type=int, default=64,\n
    \                         help=\"Batch size to use per device (For evaluation).\")\n
    \ \n      parser.add_argument(\"--grad-accum\", type=int, default=1,\n                          help=\"Gradient
    accumulation steps.\")\n  \n      parser.add_argument(\"--output-dir\", type=str,
    default=\"/tmp\",\n                          help=\"Path to output directory.\")\n
    \ \n      parser.add_argument(\"--bucket\", type=str,\n                          help=\"Bucket
    for results and checkpoints storage\")\n  \n      parser.add_argument(\"--no-grad-ckpt\",
    action=\"store_true\",\n                          help=\"If passed, will not use
    gradient checkpointing.\")\n  \n      parser.add_argument(\"--num-checkpoints-to-keep\",
    type=int, default=1,\n                          help=\"Number of checkpoints to
    keep, if None, all checkpoints will be kept, \"\n                              \"if
    set to n>=1, the top n checkpoint with min. evaluation perplexity \"\n                              \"will
    be kept.\")\n  \n      parser.add_argument(\"--stop-perplexity\", type=float,
    default=0,\n                          help=\"Target perplexity to reach after
    which to stop training. Default is 0. \"\n                              \"If 0,
    training will not stop on perplexity.\")\n  \n      parser.add_argument(\"--as-test\",
    action=\"store_true\",\n                          help=\"If passed, will run the
    script in test mode.\")\n  \n      args = parser.parse_args()\n  \n      return
    args\n  \n  \n  def main():\n      args = parse_args()\n  \n      if not args.output_dir:\n
    \         raise ValueError(\"--output-dir must be specified\")\n  \n      # update
    the config with args so that we have access to them.\n      config = vars(args)\n
    \     config.update(\n          **{\n              \"lr\": args.lr,\n              \"num_epochs\":
    args.num_epochs,\n              \"seed\": 42,\n              \"batch_size\": args.batch_size_per_device,\n
    \             \"gradient_accumulation_steps\": args.grad_accum,\n              \"model_name\":
    args.model_name,\n              \"block_size\": args.ctx_len,\n              \"eval_batch_size\":
    args.eval_batch_size_per_device,\n          }\n      )\n  \n      # Add LoRA config
    if needed\n      if args.lora:\n          with open(args.lora_config, \"r\") as
    json_file:\n              lora_config = json.load(json_file)\n          config[\"lora_config\"]
    = lora_config\n  \n      # Add deepspeed plugin to the config\n      ds_plugin
    = DeepSpeedPlugin(hf_ds_config=config.get(\"ds_config\"))\n      config.update(ds_plugin=ds_plugin)\n
    \ \n      ray.init()\n  \n      dataset = load_gsm8k_dataset()\n  \n      data_hf_chat_template
    = process_gsm8k_hf_chat_template(dataset)\n      train_ds = ray.data.from_items(data_hf_chat_template[\"train\"])\n
    \     valid_ds = ray.data.from_items(data_hf_chat_template[\"test\"])\n  \n      #
    Config file\n      chat_template = None\n      special_tokens = None\n      if
    os.path.isfile(args.dataset_config):\n          with open(args.dataset_config,
    \"r\") as json_file:\n              dataset_config = json.load(json_file)\n              chat_template
    = dataset_config.get(\"chat_template\", None)\n              special_tokens =
    dataset_config.get(\"special_tokens\", None)\n  \n      trial_name = f\"{args.model_name}\".split(\"/\")[-1]\n
    \     if args.lora:\n          trial_name += \"-lora\"\n  \n      trainer = TorchTrainer(\n
    \         training_function,\n          train_loop_config={\n              \"config\":
    config,\n              \"args\": vars(args),\n              \"chat_template\":
    chat_template,\n              \"special_tokens\": special_tokens,\n          },\n
    \         run_config=train.RunConfig(\n              storage_path=args.bucket,\n
    \             checkpoint_config=train.CheckpointConfig(\n                  num_to_keep=args.num_checkpoints_to_keep,\n
    \                 checkpoint_score_attribute=\"perplexity\",\n                  checkpoint_score_order=\"min\",\n
    \             ),\n              storage_filesystem=GcsFileSystem(default_bucket_location=\"europe-west4\",\n
    \             project_id=\"silogen-dev\",)\n              # storage_filesystem=S3FileSystem(\n
    \             # access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n              #
    secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n              # endpoint_override='https://storage.googleapis.com/',\n
    \             # region=\"auto\",\n              # request_timeout=3600,\n              #
    connect_timeout=3600,\n              # retry_strategy=AwsDefaultS3RetryStrategy(max_attempts=500)\n
    \             # ),)\n          ),\n          scaling_config=train.ScalingConfig(\n
    \             num_workers=args.num_devices,\n              use_gpu=True,\n              resources_per_worker={\"GPU\":
    1},\n          ),\n          datasets={\"train\": train_ds, \"valid\": valid_ds},\n
    \         dataset_config=ray.train.DataConfig(datasets_to_split=[\"train\", \"valid\"]),\n
    \     )\n  \n      result: train.Result = trainer.fit()\n      # `best_checkpoints`
    are sorted in increasing score order.\n      # (Ex: in this case, negative perplexity,
    since we set `checkpoint_score_order=min`)\n      best_checkpoint, best_checkpoint_metrics
    = result.best_checkpoints[-1]\n  \n      print(\"Results are stored at:\")\n      print(result.path)\n
    \     print(\"Best checkpoint is stored at:\")\n      print(best_checkpoint)\n
    \     print(f\"With perplexity: {best_checkpoint_metrics['perplexity']}\")\n  \n
    \ \n  if __name__ == \"__main__\":\n      main()\n  "
  zero_3_offload_optim_param.json: "|\n  {\n  \t\"fp16\": {\n  \t\t\"enabled\": false\n
    \ \t},\n  \t\"bf16\": {\n  \t\t\"enabled\": true\n  \t},\n  \t\"zero_optimization\":
    {\n  \t\t\"stage\": 3,\n  \t\t\"offload_optimizer\": {\n  \t\t\"device\": \"cpu\",\n
    \ \t\t\"pin_memory\": false\n  \t\t},\n  \t\t\"offload_param\": {\n  \t\t\"device\":
    \"cpu\",\n  \t\t\"pin_memory\": false\n  \t\t},\n  \t\t\"overlap_comm\": true,\n
    \ \t\t\"contiguous_gradients\": true,\n  \t\t\"reduce_bucket_size\": \"auto\",\n
    \ \t\t\"stage3_prefetch_bucket_size\": \"auto\",\n  \t\t\"stage3_param_persistence_threshold\":
    \"auto\",\n  \t\t\"gather_16bit_weights_on_model_save\": true,\n  \t\t\"round_robin_gradients\":
    true\n  \t},\n  \t\"gradient_accumulation_steps\": \"auto\",\n  \t\"gradient_clipping\":
    \"auto\",\n  \t\"steps_per_print\": 10,\n  \t\"train_batch_size\": \"auto\",\n
    \ \t\"train_micro_batch_size_per_gpu\": \"auto\",\n  \t\"wall_clock_breakdown\":
    false\n  }"
Kind: ConfigMap
Metadata:
  name: ray-job-config
  namespace: default
