apiVersion: kaiwo.silogen.ai/v1alpha1
kind: KaiwoJob
metadata:
  name: multinode-rlhf-example
spec:
  user: test@amd.com
  gpus: 12
  ray: true
  entrypoint: |
    python3 -m openrlhf.cli.train_ppo_ray \
    --ref_num_nodes 1 \
    --ref_num_gpus_per_node 2 \
    --reward_num_nodes 1 \
    --reward_num_gpus_per_node 2 \
    --critic_num_nodes 1 \
    --critic_num_gpus_per_node 2 \
    --actor_num_nodes 1 \
    --actor_num_gpus_per_node 2 \
    --vllm_num_engines 2 \
    --vllm_tensor_parallel_size 2 \
    --colocate_critic_reward \
    --colocate_actor_ref \
    --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
    --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
    --save_path checkpoint/llama3-8b-rlhf \
    --micro_train_batch_size 4 \
    --train_batch_size 8 \
    --micro_rollout_batch_size 8 \
    --rollout_batch_size 64 \
    --max_samples 100000 \
    --max_epochs 1 \
    --prompt_max_len 1024 \
    --generate_max_len 1024 \
    --zero_stage 3 \
    --bf16 \
    --actor_learning_rate 5e-7 \
    --critic_learning_rate 9e-6 \
    --init_kl_coef 0.01 \
    --prompt_data OpenRLHF/prompt-collection-v0.1 \
    --input_key context_messages \
    --apply_chat_template \
    --packing_samples \
    --normalize_reward \
    --adam_offload \
    --flash_attn \
    --gradient_checkpointing
  env:
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: hf-token
        key: hf-token
  storage:
    storageEnabled: true
    storageClassName: multinode
    huggingFace:
      storageSize: "100Gi"
      mountPath: "/hf_cache" # Also sets the env var HF_HOME to this value in each container
      preCacheRepos:
      - repoId: OpenRLHF/Llama-3-8b-sft-mixture
        files: []
      - repoId: OpenRLHF/Llama-3-8b-rm-mixture
        files: []
