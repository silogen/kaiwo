apiVersion: kaiwo.silogen.ai/v1alpha1
kind: KaiwoJob
metadata:
  name: batch-vllm
spec:
  user: test@amd.com
  # gpus-per-replica: 4
  # replicas: 2
  gpus: 8
  ray: true
  entrypoint: python code/main.py
  env:
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: hf-token
        key: hf-token
  - name: MODEL_ID
    value: meta-llama/Llama-3.1-8B-Instruct
  storage:
    storageEnabled: true
    storageClassName: multinode
    data:
      storageSize: 10Mi
      mountPath: /workload
      download:
        git:
        - repository: https://github.com/silogen/kaiwo.git
          path: workloads/inference/LLMs/offline-inference/vllm-batch-single-multinode
          targetPath: code
    huggingFace:
      storageSize: "100Gi"
      mountPath: "/.cache/huggingface"  # Also sets the env var HF_HOME to this value in each container
      preCacheRepos:
      - repoId: siloai-eai-test/private-test
        files:
        - "README.md"
      # preCacheRepos:
      # - repoId: meta-llama/Llama-3.1-8B-Instruct
      #   files: []
