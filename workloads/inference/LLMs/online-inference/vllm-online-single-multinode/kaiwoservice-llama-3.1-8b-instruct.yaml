apiVersion: kaiwo.silogen.ai/v1alpha1
kind: KaiwoService
metadata:
  name: online-inference-vllm-example9
spec:
  user: test@amd.com
  gpus: 8
  ray: true
  serveConfigV2: |
    applications:
    - name: llm
      route_prefix: /
      import_path: workloads.inference.LLMs.online-inference.vllm-online-single-multinode:deployment
      runtime_env:
          working_dir: "https://github.com/silogen/kaiwo/archive/8b170070f7cc2d9b33f1511c9ea96eda288b08e0.zip"
      deployments:
      - name: VLLMDeployment
        autoscaling_config:
          metrics_interval_s: 0.2
          look_back_period_s: 2
          downscale_delay_s: 600
          upscale_delay_s: 30
          target_num_ongoing_requests_per_replica: 20
        graceful_shutdown_timeout_s: 5
        max_concurrent_queries: 100
  env:
  - name: NCCL_P2P_DISABLE
    value: "1"
  - name: MODEL_ID
    value: "meta-llama/Llama-3.1-8B-Instruct"
  - name: GPU_MEMORY_UTILIZATION
    value: "0.9"
  - name: PLACEMENT_STRATEGY
    value: "PACK"
  - name: MAX_MODEL_LEN
    value: "8192"
  - name: MAX_NUM_SEQ
    value: "4"
  - name: MAX_NUM_BATCHED_TOKENS
    value: "32768"
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: hf-token
        key: hf-token
  storage:
    storageEnabled: true
    storageClassName: multinode
    huggingFace:
      storageSize: "100Gi"
      mountPath: "/hf_cache" # Also sets the env var HF_HOME to this value in each container
      preCacheRepos:
      - repoId: meta-llama/Llama-3.1-8B-Instruct
        files: []
