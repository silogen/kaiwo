// Prometheus remote write endpoint
prometheus.remote_write "default" {
  endpoint {
    url = "http://kube-prometheus-stack-prometheus.test-observability.svc.cluster.local:9090/api/v1/write"
  }
}

// ============================================================================
// AMD GPU Metrics Collection
// ============================================================================
// Scrape AMD GPU OpenTelemetry collector
prometheus.scrape "amd_gpu_metrics" {
  targets = [
    {
      "__address__" = "default-metrics-exporter.kube-amd-gpu.svc.cluster.local:5000",
      "job" = "amd-gpu-exporter",
      "gpu_type" = "amd",
    },
  ]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "10s"
}

// ============================================================================
// NVIDIA GPU Metrics Collection (Mock for testing)
// ============================================================================
// Scrape NVIDIA DCGM exporter (mock in test environments)
prometheus.scrape "nvidia_gpu_metrics" {
  targets = discovery.kubernetes.services.targets
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "10s"
}

discovery.kubernetes "services" {
  role = "service"
  namespaces {
    names = ["gpu-operator"]
  }
  selectors {
    role = "service"
    label = "app=nvidia-dcgm-exporter"
  }
}

// ============================================================================
// Host Node Metrics - kubelet/cAdvisor
// ============================================================================
discovery.kubernetes "nodes" {
  role = "node"
}

// Scrape kubelet metrics
prometheus.scrape "kubelet" {
  targets    = discovery.kubernetes.nodes.targets
  scheme     = "https"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    insecure_skip_verify = true
  }
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
}

// Scrape cAdvisor metrics from kubelet
prometheus.scrape "cadvisor" {
  targets    = discovery.kubernetes.nodes.targets
  scheme     = "https"
  metrics_path = "/metrics/cadvisor"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    insecure_skip_verify = true
  }
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
}

// ============================================================================
// Node Exporter Metrics - DISABLED
// Note: Using existing node-exporter from monitoring namespace
// ============================================================================

// ============================================================================
// vCluster Audit Logs Collection
// ============================================================================
// Loki remote write endpoint (direct to Loki, bypassing crashlooping gateway)
loki.write "loki" {
  endpoint {
    url = "http://loki-0.loki-headless.test-observability.svc.cluster.local:3100/loki/api/v1/push"
  }
}

// ============================================================================
// Discover vCluster Namespaces (to extract labels)
// ============================================================================
discovery.kubernetes "vcluster_namespaces" {
  role = "namespace"
  selectors {
    role = "namespace"
    label = "vcluster-namespace"  // Only namespaces with this label
  }
}

// ============================================================================
// vCluster Control Plane Pod Logs (audit logs + syncer logs)
// ============================================================================
discovery.kubernetes "vcluster_pods" {
  role = "pod"
  selectors {
    role = "pod"
    label = "app=vcluster"
  }
}

discovery.relabel "vcluster_logs" {
  targets = discovery.kubernetes.vcluster_pods.targets

  // Extract namespace as vcluster_namespace label
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    target_label  = "vcluster_namespace"
  }

  // Extract namespace labels (github-run-id, etc.)
  rule {
    source_labels = ["__meta_kubernetes_namespace_label_github_run_id"]
    target_label  = "run_id"
  }

  rule {
    source_labels = ["__meta_kubernetes_namespace_label_github_run_attempt"]
    target_label  = "run_attempt"
  }

  rule {
    source_labels = ["__meta_kubernetes_namespace_label_installer"]
    target_label  = "installer"
  }

  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    target_label  = "pod"
  }

  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    target_label  = "container"
  }
}

// ============================================================================
// vCluster Workload Pod Logs (synced pods from inside vCluster)
// ============================================================================
discovery.kubernetes "vcluster_workload_pods" {
  role = "pod"
  // These are synced pods that have the vcluster.loft.sh labels
  selectors {
    role = "pod"
    label = "vcluster.loft.sh/managed-by"
  }
}

discovery.relabel "vcluster_workload_logs" {
  targets = discovery.kubernetes.vcluster_workload_pods.targets

  // Extract vCluster namespace
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    target_label  = "vcluster_namespace"
  }

  // Extract namespace labels
  rule {
    source_labels = ["__meta_kubernetes_namespace_label_github_run_id"]
    target_label  = "run_id"
  }

  rule {
    source_labels = ["__meta_kubernetes_namespace_label_github_run_attempt"]
    target_label  = "run_attempt"
  }

  rule {
    source_labels = ["__meta_kubernetes_namespace_label_installer"]
    target_label  = "installer"
  }

  // Extract original vCluster namespace from label
  rule {
    source_labels = ["__meta_kubernetes_pod_label_vcluster_loft_sh_namespace"]
    target_label  = "workload_namespace"
  }

  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    target_label  = "pod"
  }

  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    target_label  = "container"
  }

  rule {
    source_labels = ["__meta_kubernetes_pod_label_app"]
    target_label  = "app"
  }
}

// Collect vCluster control plane logs (for audit logs)
loki.source.kubernetes "vcluster_logs" {
  targets    = discovery.relabel.vcluster_logs.output
  forward_to = [loki.process.audit_logs.receiver]
}

// Collect vCluster workload pod logs (includes event-exporter)
loki.source.kubernetes "vcluster_workload_logs" {
  targets    = discovery.relabel.vcluster_workload_logs.output
  forward_to = [
    loki.process.workload_logs.receiver,
    loki.process.event_exporter_logs.receiver,
  ]
}

// Process and parse audit logs
loki.process "audit_logs" {
  forward_to = [loki.write.loki.receiver]

  // Extract run metadata from vcluster_namespace
  // Format: test-{installer}-{run_id}-{run_attempt}
  // Example: test-manual-999-1 or test-ci-550e8400-e29b-41d4-a89f-446657440000-1
  stage.regex {
    source     = "vcluster_namespace"
    expression = "^test-(?P<installer>[^-]+)-(?P<run_id>.+?)-(?P<run_attempt>\\d+)$"
  }

  // Promote run metadata to labels
  stage.labels {
    values = {
      installer   = "",
      run_id      = "",
      run_attempt = "",
    }
  }

  // Drop logs that don't match the new namespace format
  stage.match {
    selector = "{installer=\"\"}"
    action   = "drop"
  }

  // Only process lines from the vcluster syncer that contain an audit Event JSON
  stage.match {
    selector = "{job=\"loki.source.kubernetes.vcluster_logs\", container=\"syncer\"}"

    // 1) Try to capture UNESCAPED JSON:  {"kind":"Event", ... }
    stage.regex {
      expression = "(?P<audit_unescaped>\\{\\s*\\\"kind\\\"\\s*:\\s*\\\"Event\\\"[\\s\\S]*\\})"
    }

    // 2) If logs are DOUBLE-ESCAPED (\"kind\":\"Event\" ...), capture that as well
    //    This will fire when 1) misses; both can coexist safely.
    stage.regex {
      expression = "(?P<audit_escaped>\\{\\\\\\\"kind\\\\\\\"\\s*:\\s*\\\\\\\"Event\\\\\\\"[\\s\\S]*?\\})"
    }

    // 3) Prefer unescaped; otherwise unescape the escaped JSON string
    stage.template {
      source   = "audit_json"
      template = "{{ if .audit_unescaped }}{{ .audit_unescaped }}{{ end }}"
    }

    // 4) Bail out early if we still didn't get JSON (keeps noise out)
    stage.match {
      selector = "{audit_json=\"\"}"
      action   = "drop"
    }

    // 5) Extract fields using regex from the audit_json
    stage.regex {
      source     = "audit_json"
      expression = "\"verb\"\\s*:\\s*\"(?P<verb>[^\"]+)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"username\"\\s*:\\s*\"(?P<user>[^\"]+)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"objectRef\"[^}]*\"namespace\"\\s*:\\s*\"(?P<namespace>[^\"]*)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"objectRef\"[^}]*\"resource\"\\s*:\\s*\"(?P<resource>[^\"]*)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"objectRef\"[^}]*\"name\"\\s*:\\s*\"(?P<name>[^\"]*)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"objectRef\"[^}]*\"apiVersion\"\\s*:\\s*\"(?P<api_version>[^\"]*)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"objectRef\"[^}]*\"apiGroup\"\\s*:\\s*\"(?P<api_group>[^\"]*)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"stage\"\\s*:\\s*\"(?P<audit_stage>[^\"]+)\""
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"code\"\\s*:\\s*(?P<status_code>\\d+)"
    }

    stage.regex {
      source     = "audit_json"
      expression = "\"requestURI\"\\s*:\\s*\"(?P<request_uri>[^\"]+)\""
    }

    // 7) Keep the raw (now-clean) JSON for details
    stage.template {
      source   = "audit_json_for_details"
      template = "{{ .audit_json }}"
    }

    // 8) Map to a level
    stage.template {
      source   = "level"
      template = "{{ if .verb }}{{ if eq .verb \"delete\" }}warning{{ else if and .status_code (ge (.status_code | int) 400) }}error{{ else }}info{{ end }}{{ else }}info{{ end }}"
    }

    // 9) Human-readable message
    stage.template {
      source   = "formatted_message"
      template = "{{ if and .verb .resource .name .user }}{{ .verb }} {{ .resource }}/{{ .name }} by {{ .user }}{{ else }}audit event{{ end }}"
    }

    // 10) Unified JSON (note: 'kind' uses resource; plural is expected for K8s resources)
    stage.template {
      source   = "unified_log"
      template = "{\"source\":\"k8s_audit_log\",\"level\":\"{{ if .level }}{{ .level }}{{ else }}info{{ end }}\",\"action\":\"{{ .verb }}\",\"message\":\"{{ .formatted_message }}\",\"object\":{\"name\":\"{{ .name }}\",\"namespace\":\"{{ .namespace }}\",\"kind\":\"{{ .resource }}\",\"apiVersion\":\"{{ .api_version }}\",\"apiGroup\":\"{{ .api_group }}\"},\"pod\":\"\",\"container\":\"\",\"timestamp\":\"{{ .timestamp }}\",\"details\":{{ .audit_json_for_details }}}"
    }

    stage.labels { values = { level = "" } }

    stage.static_labels { values = { source = "k8s_audit_log" } }

    stage.output { source = "unified_log" }
  }

}

// ============================================================================
// Process vCluster Workload Logs - Unified Schema
// ============================================================================
loki.process "workload_logs" {
  forward_to = [loki.write.loki.receiver]

  // Extract run metadata from vcluster_namespace
  // Format: test-{installer}-{run_id}-{run_attempt}
  // Example: test-manual-999-1 or test-ci-550e8400-e29b-41d4-a89f-446657440000-1
  stage.regex {
    source     = "vcluster_namespace"
    expression = "^test-(?P<installer>[^-]+)-(?P<run_id>.+?)-(?P<run_attempt>\\d+)$"
  }

  // Promote run metadata to labels
  stage.labels {
    values = {
      installer   = "",
      run_id      = "",
      run_attempt = "",
    }
  }

  // Drop logs that don't match the new namespace format
  stage.match {
    selector = "{installer=\"\"}"
    action   = "drop"
  }

  // Drop noisy health check logs early
  stage.match {
    selector = "{app=\"kube-probe\"}"
    action   = "drop"
  }

  // Store original log line for message
  stage.template {
    source = "original_line"
    template = "{{ .Entry }}"
  }

  // Try to extract JSON logs if present
  stage.json {
    expressions = {
      level = "level",
      msg   = "msg",
      message = "message",
      time  = "time",
      timestamp = "timestamp",
      name  = "name",
      namespace = "namespace",
      kind  = "kind",
      apiVersion = "apiVersion",
    }
  }

  // Normalize level field (default to "info" if not present)
  stage.template {
    source = "level"
    template = "{{ if .level }}{{ .level }}{{ else }}info{{ end }}"
  }

  // Build log message (use msg or message field if present, otherwise use original line)
  stage.template {
    source = "log_message"
    template = "{{ if .msg }}{{ .msg }}{{ else if .message }}{{ .message }}{{ else }}{{ .original_line }}{{ end }}"
  }

  // Build unified JSON output (without raw for pod logs to avoid escaping issues)
  stage.template {
    source = "unified_log"
    template = "{\"source\":\"pod_log\",\"level\":\"{{ if .level }}{{ .level }}{{ else }}info{{ end }}\",\"action\":\"log\",\"message\":\"{{ if .log_message }}{{ .log_message }}{{ end }}\",\"object\":{},\"pod\":\"{{ if .pod }}{{ .pod }}{{ end }}\",\"container\":\"{{ if .container }}{{ .container }}{{ end }}\",\"timestamp\":\"{{ if .timestamp }}{{ .timestamp }}{{ end }}\",\"details\":{}}"
  }

  // Promote log level to label
  stage.labels {
    values = {
      level = "",
    }
  }

  // Add source label
  stage.static_labels {
    values = {
      source = "pod_log",
    }
  }

  // Replace the log line with unified JSON
  stage.output {
    source = "unified_log"
  }
}

// ============================================================================
// Kubernetes Events from event-exporter (inside vClusters)
// ============================================================================
// Event-exporter runs inside each vCluster and outputs events as JSON to stdout
// Alloy collects these logs like any other workload pod logs
// The event-exporter JSON already contains the original object names/namespaces

// Events are collected via the workload_logs source and processed here
loki.process "event_exporter_logs" {
  forward_to = [loki.write.loki.receiver]

  // Extract run metadata from vcluster_namespace
  // Format: test-{installer}-{run_id}-{run_attempt}
  // Example: test-manual-999-1 or test-ci-550e8400-e29b-41d4-a89f-446657440000-1
  stage.regex {
    source     = "vcluster_namespace"
    expression = "^test-(?P<installer>[^-]+)-(?P<run_id>.+?)-(?P<run_attempt>\\d+)$"
  }

  // Promote run metadata to labels
  stage.labels {
    values = {
      installer   = "",
      run_id      = "",
      run_attempt = "",
    }
  }

  // Drop logs that don't match the new namespace format
  stage.match {
    selector = "{installer=\"\"}"
    action   = "drop"
  }

  // Drop all logs that are NOT from event-exporter to prevent duplicates
  stage.match {
    selector = "{app!=\"event-exporter\"}"
    action   = "drop"
  }

  // Only process logs from event-exporter pods
  stage.match {
    selector = "{app=\"event-exporter\"}"

    // Store original log line (the full event JSON)
    stage.template {
      source = "event_json"
      template = "{{ .Entry }}"
    }

    // Parse the JSON event from event-exporter
    stage.json {
      expressions = {
        object_name        = "involvedObject.name",
        object_namespace   = "involvedObject.namespace",
        object_kind        = "involvedObject.kind",
        object_api_version = "involvedObject.apiVersion",
        reason             = "reason",
        type               = "type",
        message            = "message",
        count              = "count",
        first_timestamp    = "firstTimestamp",
        last_timestamp     = "lastTimestamp",
      }
    }

    // Map event type to standard log level
    stage.template {
      source = "level"
      template = "{{ if .type }}{{ if eq .type \"Warning\" }}warning{{ else }}info{{ end }}{{ else }}info{{ end }}"
    }

    // Use event message as-is (reason is in action field)
    stage.template {
      source = "formatted_message"
      template = "{{ if .message }}{{ .message }}{{ else }}event{{ end }}"
    }

    // Build unified JSON output - embed raw event JSON in details AND use extracted fields
    stage.template {
      source = "unified_log"
      template = "{\"source\":\"k8s_event\",\"level\":\"{{ .level }}\",\"action\":\"{{ .reason }}\",\"message\":\"{{ .formatted_message }}\",\"object\":{\"name\":\"{{ .object_name }}\",\"namespace\":\"{{ .object_namespace }}\",\"kind\":\"{{ .object_kind }}\",\"apiVersion\":\"{{ .object_api_version }}\"},\"pod\":\"\",\"container\":\"\",\"timestamp\":\"{{ .timestamp }}\",\"details\":{{ .event_json }}}"
    }

    // Promote level to label
    stage.labels {
      values = {
        level = "",
      }
    }

    // Add source label
    stage.static_labels {
      values = {
        source = "k8s_event",
      }
    }

    // Replace the log line with unified JSON
    stage.output {
      source = "unified_log"
    }
  }
}